{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\n",
    "\n",
    "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n",
    "\n",
    "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\n",
    "\n",
    "For all exercises in this chapter, we've imported the Sequential model constructor, the Dense layer constructor, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../datasets/hourly_wages.csv')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 534 entries, 0 to 533\n",
      "Data columns (total 10 columns):\n",
      "wage_per_hour     534 non-null float64\n",
      "union             534 non-null int64\n",
      "education_yrs     534 non-null int64\n",
      "experience_yrs    534 non-null int64\n",
      "age               534 non-null int64\n",
      "female            534 non-null int64\n",
      "marr              534 non-null int64\n",
      "south             534 non-null int64\n",
      "manufacturing     534 non-null int64\n",
      "construction      534 non-null int64\n",
      "dtypes: float64(1), int64(9)\n",
      "memory usage: 41.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = data.drop(\"wage_per_hour\",axis=1).values\n",
    "target = data[\"wage_per_hour\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "534/534 [==============================] - 0s - loss: 22.4969     \n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s - loss: 21.7966     \n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s - loss: 22.0746     \n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s - loss: 21.6728     \n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s - loss: 22.1628     \n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s - loss: 21.1527      ETA: 0s - loss: 1\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s - loss: 21.3114     \n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s - loss: 20.7995     \n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s - loss: 20.8234      ETA: 0s - loss: 18.\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s - loss: 20.9645      ETA: 0s - loss:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f9746a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x=predictors,y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15624.928386079619"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((model.predict(predictors) - target)**2).sum()/len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n",
      "Epoch 1/1\n",
      "534/534 [==============================] - 0s - loss: 392.4011     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fe526d8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)\n",
    "\n",
    "model.fit(x=predictors,y=target,nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59155.543261891617"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((model.predict(predictors) - target)**2).sum()/len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/titanic/train.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.drop(['PassengerId','Cabin','Name','Ticket'],axis=1, inplace=True)\n",
    "df.head()\n",
    "\n",
    "\n",
    "df['Age'].fillna(23,inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "df.head()\n",
    "\n",
    "predictors = df.drop('Survived',axis=1).values\n",
    "target = pd.get_dummies(df['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ..., \n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "889/889 [==============================] - 0s - loss: 2.6129 - acc: 0.5782     \n",
      "Epoch 2/20\n",
      "889/889 [==============================] - 0s - loss: 1.4158 - acc: 0.6310     \n",
      "Epoch 3/20\n",
      "889/889 [==============================] - 0s - loss: 1.0311 - acc: 0.6198     \n",
      "Epoch 4/20\n",
      "889/889 [==============================] - 0s - loss: 0.9437 - acc: 0.6198     \n",
      "Epoch 5/20\n",
      "889/889 [==============================] - 0s - loss: 0.7050 - acc: 0.6592     - ETA: 0s - loss: 0.6816 - acc:\n",
      "Epoch 6/20\n",
      "889/889 [==============================] - 0s - loss: 0.6304 - acc: 0.6715     \n",
      "Epoch 7/20\n",
      "889/889 [==============================] - 0s - loss: 0.6347 - acc: 0.7042     \n",
      "Epoch 8/20\n",
      "889/889 [==============================] - 0s - loss: 0.6313 - acc: 0.6918     - ETA: 0s - loss: 0.6780 - a\n",
      "Epoch 9/20\n",
      "889/889 [==============================] - 0s - loss: 0.5917 - acc: 0.6940     \n",
      "Epoch 10/20\n",
      "889/889 [==============================] - ETA: 0s - loss: 0.5971 - acc: 0.6875- ETA: 0s - loss: 0.6221 - acc: - ETA: 0s - loss: 0.5983 - acc: 0. - 0s - loss: 0.5960 - acc: 0.6873     \n",
      "Epoch 11/20\n",
      "889/889 [==============================] - 0s - loss: 0.5930 - acc: 0.7109     \n",
      "Epoch 12/20\n",
      "889/889 [==============================] - 0s - loss: 0.6048 - acc: 0.6963     \n",
      "Epoch 13/20\n",
      "889/889 [==============================] - 0s - loss: 0.6020 - acc: 0.6749     \n",
      "Epoch 14/20\n",
      "889/889 [==============================] - 0s - loss: 0.5856 - acc: 0.6940     - ETA: 0s - loss: 0.5590 - acc: 0.7\n",
      "Epoch 15/20\n",
      "889/889 [==============================] - 0s - loss: 0.5766 - acc: 0.6985     - ETA: 0s - loss: 0.6082 -\n",
      "Epoch 16/20\n",
      "889/889 [==============================] - 0s - loss: 0.5770 - acc: 0.7132     - ETA: 0s - loss: 0.5487 -\n",
      "Epoch 17/20\n",
      "889/889 [==============================] - 0s - loss: 0.5764 - acc: 0.6985     \n",
      "Epoch 18/20\n",
      "889/889 [==============================] - 0s - loss: 0.5735 - acc: 0.7030     \n",
      "Epoch 19/20\n",
      "889/889 [==============================] - 0s - loss: 0.5853 - acc: 0.6940     \n",
      "Epoch 20/20\n",
      "889/889 [==============================] - 0s - loss: 0.5716 - acc: 0.7075     - ETA: 0s - loss: 0.5713 - acc: 0.69\n",
      "(889, 2)\n",
      "[[ 0.81960958  0.18039043]\n",
      " [ 0.25088909  0.74911088]\n",
      " [ 0.73864943  0.26135063]\n",
      " ..., \n",
      " [ 0.62831491  0.37168509]\n",
      " [ 0.46156013  0.53843987]\n",
      " [ 0.84503591  0.15496407]]\n",
      "[ 0.24452552  0.65972161  0.24103357  0.57466471  0.16234338  0.28856847\n",
      "  0.43702146  0.39693186  0.31352714  0.52641058  0.44482788  0.39967725\n",
      "  0.24174732  0.40936172  0.30934456  0.26552469  0.4419198   0.32824129\n",
      "  0.32551014  0.29175848  0.35398933  0.29608116  0.35183978  0.48605636\n",
      "  0.44414201  0.45557067  0.256533    0.71176797  0.32650667  0.2280442\n",
      "  0.45849553  0.70880836  0.32573751  0.08993316  0.67397892  0.50493091\n",
      "  0.25662109  0.23775753  0.38582194  0.38875425  0.21334064  0.38713437\n",
      "  0.27084488  0.50967574  0.33860293  0.22991471  0.34243259  0.32573751\n",
      "  0.36378255  0.39126411  0.44005141  0.23465861  0.65085554  0.41087699\n",
      "  0.47286922  0.50388557  0.35781282  0.21967378  0.52424097  0.47038451\n",
      "  0.26375911  0.63459098  0.43017057  0.4866038   0.33758104  0.32593429\n",
      "  0.24716452  0.35737023  0.25956813  0.27531299  0.51975381  0.60446125\n",
      "  0.31151676  0.5024246   0.21315072  0.2280442   0.22991471  0.47664818\n",
      "  0.28455865  0.24570894  0.22251962  0.3259607   0.53894877  0.37043095\n",
      "  0.32294214  0.50230801  0.22991471  0.74699497  0.22606193  0.19616936\n",
      "  0.23928991  0.53808933  0.35290849  0.05636061  0.22991471  0.40107471\n",
      "  0.64795178  0.40931547  0.34514338  0.23275724  0.2280442   0.66194874\n",
      "  0.18357064  0.19704852  0.19936401  0.25798523  0.2265864   0.14231092\n",
      "  0.42967117  0.48784107  0.40941986  0.23381329  0.31000143  0.39810047\n",
      "  0.23620459  0.05043425  0.34584835  0.74101603  0.46529564  0.58574313\n",
      "  0.22991471  0.40046525  0.33763602  0.60743755  0.36705062  0.27874121\n",
      "  0.21001759  0.42581809  0.09590964  0.20404814  0.22937724  0.26088569\n",
      "  0.41087699  0.32226089  0.37155473  0.55083668  0.53044599  0.28861278\n",
      "  0.66900271  0.38163772  0.25512064  0.33722886  0.28828621  0.33720359\n",
      "  0.49979874  0.20356165  0.504412    0.39300013  0.2565645   0.19002211\n",
      "  0.65207011  0.07290079  0.25957519  0.21943487  0.56445062  0.34692073\n",
      "  0.19038747  0.23745029  0.52030188  0.24802823  0.33146858  0.20926145\n",
      "  0.26204425  0.43831721  0.45151585  0.64137465  0.39540324  0.45446807\n",
      "  0.51458377  0.39384392  0.44249874  0.42919567  0.23620459  0.42462239\n",
      "  0.28062573  0.37856776  0.47401115  0.307565    0.05203122  0.55919194\n",
      "  0.37158135  0.4389511   0.47585866  0.48521066  0.56642371  0.38208032\n",
      "  0.40097004  0.30417892  0.15339467  0.33916137  0.34957454  0.29218128\n",
      "  0.46454957  0.48771322  0.8523683   0.27874121  0.13750775  0.32573751\n",
      "  0.36400321  0.22818007  0.52030188  0.13800409  0.1201295   0.24986292\n",
      "  0.4369629   0.27444857  0.32903594  0.34702322  0.45020896  0.20835033\n",
      "  0.37960589  0.22411078  0.307565    0.29531974  0.75647384  0.23705187\n",
      "  0.36379161  0.7007466   0.29035446  0.2611731   0.31633812  0.0874683\n",
      "  0.2280442   0.67926288  0.25019306  0.32533434  0.22987191  0.35979101\n",
      "  0.41516677  0.69598359  0.19144699  0.16254964  0.46963462  0.31056255\n",
      "  0.24840872  0.351311    0.5382809   0.32533434  0.29482976  0.35883665\n",
      "  0.38208032  0.29616699  0.22226998  0.21021067  0.68637305  0.24315898\n",
      "  0.40031657  0.54613543  0.32092252  0.21826051  0.29967502  0.34681788\n",
      "  0.281275    0.3255204   0.36254036  0.69978547  0.71587884  0.87292713\n",
      "  0.37670815  0.27874121  0.43026972  0.61609894  0.08028079  0.32573751\n",
      "  0.24672271  0.45415032  0.23909923  0.86404771  0.78717268  0.48435977\n",
      "  0.08177715  0.36424771  0.47295463  0.32573751  0.59024984  0.12378142\n",
      "  0.11810912  0.44342601  0.33950669  0.06413905  0.19863522  0.2940518\n",
      "  0.24578255  0.45491216  0.21807572  0.21616995  0.23192063  0.2565645\n",
      "  0.3287372   0.69374436  0.66582978  0.31174323  0.26121041  0.22409892\n",
      "  0.4866038   0.25309977  0.57297176  0.4816632   0.87158388  0.32573751\n",
      "  0.3808668   0.10554732  0.41205806  0.22991471  0.5494284   0.70328432\n",
      "  0.6561057   0.37963548  0.63158745  0.70773196  0.71878374  0.44773114\n",
      "  0.19936401  0.36905879  0.24014479  0.44469494  0.19797425  0.76739091\n",
      "  0.81398106  0.22411078  0.20534933  0.38604859  0.49236181  0.52030188\n",
      "  0.79321665  0.04482526  0.32705498  0.35353357  0.64337963  0.41676757\n",
      "  0.41206875  0.79495978  0.35958406  0.70605779  0.2280442   0.61149782\n",
      "  0.82112306  0.11096448  0.40687534  0.46425775  0.75243634  0.31339878\n",
      "  0.32226089  0.29043338  0.36400321  0.31095663  0.34549502  0.40272498\n",
      "  0.13513784  0.24451907  0.50171548  0.31383994  0.31152984  0.256533\n",
      "  0.22818007  0.64416748  0.32041374  0.32650667  0.32650667  0.37091973\n",
      "  0.40697014  0.2971133   0.14280976  0.34243259  0.17626218  0.61005908\n",
      "  0.29181811  0.32573751  0.68245012  0.59640455  0.25118807  0.24578255\n",
      "  0.68788433  0.43403792  0.69610161  0.24864735  0.74412304  0.20505096\n",
      "  0.24229683  0.83328199  0.50852066  0.17720471  0.56999034  0.2280442\n",
      "  0.59856093  0.44915164  0.32705498  0.27845582  0.41227847  0.77455008\n",
      "  0.23460679  0.24990028  0.69956088  0.35617045  0.23069896  0.20993918\n",
      "  0.33683851  0.31348738  0.3493239   0.13754709  0.21430679  0.30709457\n",
      "  0.28543359  0.27587622  0.35149857  0.08391372  0.44487715  0.23435026\n",
      "  0.41516677  0.2280442   0.26666582  0.74241769  0.11810912  0.11346459\n",
      "  0.25487795  0.44120318  0.41022795  0.307565    0.50129157  0.27084488\n",
      "  0.29024076  0.19315414  0.32550883  0.38878822  0.21826051  0.41785049\n",
      "  0.47829491  0.27874121  0.17921679  0.42594621  0.34549502  0.39788482\n",
      "  0.24213523  0.48749247  0.64151096  0.50039554  0.45460141  0.913104\n",
      "  0.28277323  0.40329292  0.26047578  0.23909923  0.35148171  0.23067586\n",
      "  0.54268318  0.48905674  0.39961779  0.45720288  0.40376246  0.40216479\n",
      "  0.33901632  0.44064698  0.66440684  0.22991471  0.22919796  0.33761683\n",
      "  0.6047712   0.19051929  0.27874121  0.39119276  0.1684166   0.40222502\n",
      "  0.21646151  0.22991471  0.12739876  0.11810912  0.36556196  0.2783981\n",
      "  0.45952997  0.21826051  0.15706077  0.42883578  0.40425265  0.28331882\n",
      "  0.57493037  0.35149857  0.20378698  0.22736266  0.44127071  0.46629825\n",
      "  0.11810912  0.0910449   0.07657803  0.68725854  0.41516677  0.71733958\n",
      "  0.41143203  0.19038747  0.40802586  0.33901632  0.22794004  0.39396191\n",
      "  0.50030982  0.23775753  0.31239775  0.64306164  0.29603085  0.72298086\n",
      "  0.22219366  0.26204425  0.3317509   0.32501933  0.20762898  0.64531934\n",
      "  0.64846832  0.44188163  0.45817095  0.3303515   0.52065784  0.24524625\n",
      "  0.22991471  0.40915772  0.54675871  0.2165474   0.42181748  0.3008661\n",
      "  0.39931658  0.39092329  0.17673728  0.72937655  0.23192063  0.256533\n",
      "  0.61291534  0.25662109  0.17331418  0.19051929  0.72893351  0.13754709\n",
      "  0.34833044  0.49585819  0.25662109  0.30800161  0.44770339  0.23435013\n",
      "  0.53624088  0.40097004  0.75265867  0.28656247  0.65759176  0.68429786\n",
      "  0.47754717  0.48164296  0.35517538  0.7254253   0.3373324   0.4699569\n",
      "  0.36410215  0.30948269  0.49799398  0.6436649   0.39460713  0.2798298\n",
      "  0.26366946  0.25544721  0.34681788  0.48126692  0.72895414  0.68692774\n",
      "  0.30726406  0.27874121  0.13190337  0.31632027  0.22991471  0.25487795\n",
      "  0.35590023  0.24382389  0.39450982  0.25662109  0.176073    0.10541663\n",
      "  0.46602935  0.40872121  0.32573751  0.2611731   0.33199966  0.33308062\n",
      "  0.57465416  0.35886222  0.17720471  0.48306611  0.78857946  0.3129383\n",
      "  0.49409932  0.27997923  0.66380739  0.25521314  0.61060613  0.23381329\n",
      "  0.22991471  0.14420408  0.64862573  0.09195996  0.34907842  0.35705879\n",
      "  0.32186201  0.49908718  0.03004597  0.256533    0.52025163  0.46195289\n",
      "  0.2280442   0.53378141  0.11538034  0.44852287  0.26215547  0.18778875\n",
      "  0.46136972  0.58801317  0.81720394  0.4514387   0.21453315  0.38208032\n",
      "  0.27874121  0.16234338  0.64992267  0.27248207  0.32917923  0.51325297\n",
      "  0.30475816  0.30860201  0.50823897  0.36086139  0.23532809  0.33036232\n",
      "  0.38624522  0.19479543  0.68310487  0.21146691  0.278512    0.31327212\n",
      "  0.07616981  0.44770879  0.15571234  0.47913754  0.35148171  0.17720471\n",
      "  0.3785947   0.48207742  0.31244472  0.23928991  0.68245012  0.46586329\n",
      "  0.5259763   0.45952997  0.61782289  0.24382389  0.45668417  0.22388874\n",
      "  0.24840872  0.2280442   0.48526764  0.24256328  0.32620883  0.33481506\n",
      "  0.59170318  0.2280442   0.36662367  0.32824129  0.76264566  0.78270561\n",
      "  0.1481498   0.38823301  0.14570139  0.26156148  0.59329456  0.32226089\n",
      "  0.2265864   0.11994827  0.60534167  0.44886833  0.5438416   0.07642484\n",
      "  0.30467087  0.11810912  0.24633817  0.22382186  0.30149049  0.53398764\n",
      "  0.86868107  0.32804698  0.66918945  0.25685266  0.47652107  0.39590883\n",
      "  0.54106241  0.45226303  0.27175274  0.24660365  0.689228    0.56528074\n",
      "  0.46189895  0.5259763   0.24264447  0.35301653  0.20104285  0.11538034\n",
      "  0.3256382   0.75652248  0.11821766  0.70982629  0.40494975  0.39538923\n",
      "  0.26719114  0.23611966  0.35846394  0.28467727  0.40910691  0.70949036\n",
      "  0.33758104  0.6184817   0.45817095  0.47440988  0.22220226  0.19236064\n",
      "  0.24072361  0.81562132  0.33562499  0.33417654  0.16875415  0.5336892\n",
      "  0.26269081  0.29608116  0.20414674  0.56059283  0.24954511  0.37737763\n",
      "  0.32566315  0.40239957  0.26672786  0.76968825  0.42371014  0.11810912\n",
      "  0.32824129  0.32824129  0.27796251  0.41416422  0.86581194  0.2280442\n",
      "  0.2280442   0.47868282  0.64132971  0.73602873  0.30630478  0.18267664\n",
      "  0.48140138  0.39872307  0.34529558  0.57542372  0.23177835  0.49587199\n",
      "  0.40769619  0.19791453  0.2280442   0.59004664  0.43293229  0.19761549\n",
      "  0.33720359  0.1684166   0.71632051  0.28656247  0.11457656  0.27749321\n",
      "  0.79101926  0.25495672  0.63131344  0.54207009  0.27073535  0.39340866\n",
      "  0.18432482  0.24396619  0.09599389  0.14540759  0.256533    0.38009018\n",
      "  0.24601944  0.27874121  0.42879552  0.2785697   0.83798575  0.33698869\n",
      "  0.62122643  0.44542903  0.3941085   0.20229979  0.20587023  0.26866788\n",
      "  0.44545209  0.42774662  0.64024121  0.27874121  0.4514828   0.55919194\n",
      "  0.50352365  0.21771681  0.2734845   0.42531377  0.22861455  0.21028724\n",
      "  0.37196457  0.29608116  0.41442475  0.60641378  0.41621208  0.18932627\n",
      "  0.18022799  0.0835904   0.27245376  0.2734845   0.58059967  0.21131483\n",
      "  0.29325783  0.25367936  0.47140914  0.18754575  0.15571234  0.2532503\n",
      "  0.47763848  0.09665786  0.44227773  0.70435411  0.2194417   0.0870235\n",
      "  0.30917758  0.43860617  0.26789212  0.5259763   0.47990817  0.27874121\n",
      "  0.40595022  0.44431189  0.25662109  0.22754146  0.25309616  0.71680325\n",
      "  0.2454683   0.22991471  0.5024246   0.49841669  0.24017654  0.3469139\n",
      "  0.50325167  0.16434059  0.26204425  0.11665003  0.52030188  0.19226636\n",
      "  0.45592713  0.69687468  0.43025553  0.03233881  0.46291369  0.59764212\n",
      "  0.3913748   0.31331316  0.84181887  0.38150224  0.41801733  0.25662109\n",
      "  0.24878736  0.33344474  0.42864764  0.55919194  0.32524398  0.29627925\n",
      "  0.40044978  0.5445255   0.24802628  0.3937009   0.21146691  0.53965676\n",
      "  0.20193331  0.11657792  0.42203602  0.32921445  0.26507846  0.24382389\n",
      "  0.2280442   0.67934281  0.46097645  0.17133272  0.29288945  0.2990151\n",
      "  0.20229979  0.4791545   0.31633812  0.5308007   0.43038976  0.48494047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.22524101]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify, compile, and fit the model\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(predictors, target, nb_epoch=20)\n",
    "\n",
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(predictors)\n",
    "\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "# predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 622 samples, validate on 267 samples\n",
      "Epoch 1/10\n",
      "622/622 [==============================] - 0s - loss: 0.8730 - acc: 0.6431 - val_loss: 0.5689 - val_acc: 0.7228\n",
      "Epoch 2/10\n",
      "622/622 [==============================] - 0s - loss: 0.6488 - acc: 0.6399 - val_loss: 0.5508 - val_acc: 0.7191\n",
      "Epoch 3/10\n",
      "622/622 [==============================] - 0s - loss: 0.6319 - acc: 0.6736 - val_loss: 0.6889 - val_acc: 0.6442\n",
      "Epoch 4/10\n",
      "622/622 [==============================] - 0s - loss: 0.6818 - acc: 0.6334 - val_loss: 0.5482 - val_acc: 0.7266\n",
      "Epoch 5/10\n",
      "622/622 [==============================] - ETA: 0s - loss: 0.6564 - acc: 0.666 - 0s - loss: 0.6543 - acc: 0.6672 - val_loss: 0.5195 - val_acc: 0.7453\n",
      "Epoch 6/10\n",
      "622/622 [==============================] - 0s - loss: 0.5945 - acc: 0.7042 - val_loss: 0.4891 - val_acc: 0.7453\n",
      "Epoch 7/10\n",
      "622/622 [==============================] - 0s - loss: 0.6110 - acc: 0.6752 - val_loss: 0.5320 - val_acc: 0.7528\n",
      "Epoch 8/10\n",
      "622/622 [==============================] - 0s - loss: 0.5691 - acc: 0.7267 - val_loss: 0.4888 - val_acc: 0.7453\n",
      "Epoch 9/10\n",
      "622/622 [==============================] - 0s - loss: 0.5441 - acc: 0.7283 - val_loss: 0.4781 - val_acc: 0.7528\n",
      "Epoch 10/10\n",
      "622/622 [==============================] - 0s - loss: 0.5533 - acc: 0.7444 - val_loss: 0.5235 - val_acc: 0.7566\n"
     ]
    }
   ],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = \"adam\", loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors,target,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 622 samples, validate on 267 samples\n",
      "Epoch 1/30\n",
      "622/622 [==============================] - 0s - loss: 0.7779 - acc: 0.6447 - val_loss: 0.5687 - val_acc: 0.7453\n",
      "Epoch 2/30\n",
      "622/622 [==============================] - 0s - loss: 0.7074 - acc: 0.6592 - val_loss: 0.7315 - val_acc: 0.6442\n",
      "Epoch 3/30\n",
      "622/622 [==============================] - 0s - loss: 0.6328 - acc: 0.6785 - val_loss: 0.5107 - val_acc: 0.7640\n",
      "Epoch 4/30\n",
      "622/622 [==============================] - 0s - loss: 0.6329 - acc: 0.6849 - val_loss: 0.4979 - val_acc: 0.7715\n",
      "Epoch 5/30\n",
      "622/622 [==============================] - 0s - loss: 0.6008 - acc: 0.6785 - val_loss: 0.6095 - val_acc: 0.7378\n",
      "Epoch 6/30\n",
      "622/622 [==============================] - 0s - loss: 0.6329 - acc: 0.6913 - val_loss: 0.5919 - val_acc: 0.7004- ETA: 0s - loss: 0.6367 - acc: 0.69\n",
      "Epoch 7/30\n",
      "622/622 [==============================] - 0s - loss: 0.7419 - acc: 0.6817 - val_loss: 0.5821 - val_acc: 0.75660.68 - ETA: 0s - loss: 0.7347 - acc: 0.6\n",
      "Epoch 8/30\n",
      "622/622 [==============================] - 0s - loss: 0.6785 - acc: 0.6945 - val_loss: 0.4874 - val_acc: 0.7603\n",
      "Epoch 9/30\n",
      "622/622 [==============================] - 0s - loss: 0.5981 - acc: 0.6785 - val_loss: 0.4776 - val_acc: 0.7790\n",
      "Epoch 10/30\n",
      "622/622 [==============================] - 0s - loss: 0.5646 - acc: 0.6961 - val_loss: 0.4923 - val_acc: 0.7790\n",
      "Epoch 11/30\n",
      "622/622 [==============================] - 0s - loss: 0.5640 - acc: 0.7476 - val_loss: 0.6383 - val_acc: 0.6816\n",
      "Epoch 12/30\n",
      "622/622 [==============================] - 0s - loss: 0.5796 - acc: 0.7299 - val_loss: 0.4435 - val_acc: 0.7790\n",
      "Epoch 13/30\n",
      "622/622 [==============================] - 0s - loss: 0.6039 - acc: 0.7154 - val_loss: 0.5781 - val_acc: 0.7303\n",
      "Epoch 14/30\n",
      "622/622 [==============================] - 0s - loss: 0.7393 - acc: 0.7074 - val_loss: 0.5218 - val_acc: 0.8090\n",
      "Epoch 15/30\n",
      "622/622 [==============================] - 0s - loss: 0.5941 - acc: 0.7267 - val_loss: 0.5876 - val_acc: 0.6966\n",
      "Epoch 16/30\n",
      "622/622 [==============================] - 0s - loss: 0.5612 - acc: 0.7235 - val_loss: 0.4706 - val_acc: 0.8165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123e346a0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model=None\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors,target, nb_epoch=30, validation_split=0.3, callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
